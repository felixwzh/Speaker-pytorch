{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we build our data set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: in original data, we have pid starts from 0, but here we need to set pid starts from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. vocabulary construction.\n",
    "input_path='../../PerGen/code/OpenNMT-py/src_data_full_feat_tf_resplited_review'\n",
    "\n",
    "# 1.1 read src data.\n",
    "def read_src_data(file,vocab_dict):\n",
    "    src=[]\n",
    "    with open(file,'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in lines:\n",
    "            line=line.strip().split(' ')\n",
    "            src.append(line)\n",
    "            for word in line:\n",
    "                if word not in vocab_dict:\n",
    "                    vocab_dict[word]=0\n",
    "                vocab_dict[word]+=1\n",
    "    return src,vocab_dict\n",
    "\n",
    "# 1.2 read tgt data.\n",
    "def read_tgt_data(file,vocab_dict):\n",
    "    tgt=[]\n",
    "    with open(file,'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "        for i,line in enumerate(lines):\n",
    "            line=line.strip().split(' ')\n",
    "            pid=None\n",
    "            tgt_line=[]\n",
    "            for word_pid in line:\n",
    "#                 print(word_pid)\n",
    "                word,pid=word_pid.split('￨')\n",
    "\n",
    "                tgt_line.append(word)\n",
    "                if word not in vocab_dict:\n",
    "                    vocab_dict[word]=0\n",
    "                vocab_dict[word]+=1\n",
    "            tgt.append({'line':tgt_line,'pid':int(pid)+1})\n",
    "            \n",
    "            #FIXME: in original data, we have pid starts from 0, but here we need to set pid starts from 1\n",
    "            \n",
    "    return tgt,vocab_dict\n",
    "def build_vocab(vocab_dict,min_frequency):\n",
    "    ## we need to filter out those unfrequent words out.\n",
    "    vocab_index_dict={}\n",
    "    index_vocab_dict={}\n",
    "    # 1. sort the vocab\n",
    "    sorted_vocab=sorted(vocab_dict.items(), key=lambda item:item[1],reverse=True)\n",
    "    \n",
    "    # 2. build the index \n",
    "    UNKnown_index=1\n",
    "    index_vocab_dict[UNKnown_index]='UNKnown'\n",
    "    index=2\n",
    "    for word,frequency in sorted_vocab:\n",
    "        if frequency>min_frequency:\n",
    "            vocab_index_dict[word]=index\n",
    "            index_vocab_dict[index]=word\n",
    "            index+=1\n",
    "        else:\n",
    "            vocab_index_dict[word]=UNKnown_index\n",
    "    print(\"words num in vocab:\",index-1)\n",
    "    return vocab_index_dict,index_vocab_dict\n",
    "        \n",
    "def encode_sentence(sent,vocab_index_dict):\n",
    "    codes=[]\n",
    "    for word in sent:\n",
    "        index=vocab_index_dict[word]\n",
    "        codes.append(index)\n",
    "    assert len(codes)==len(sent)\n",
    "    return codes\n",
    "\n",
    "def output_one_set(src,tgt,file):\n",
    "    assert len(src)==len(tgt)\n",
    "    with open(file,'w') as fout:\n",
    "        for i in range(len(src)):\n",
    "            src_sent=src[i]\n",
    "            pair=tgt[i]\n",
    "            tgt_sent=pair['line']\n",
    "            tgt_pid=pair['pid']\n",
    "            src_sent=encode_sentence(src_sent,vocab_index_dict)\n",
    "            tgt_sent=encode_sentence(tgt_sent,vocab_index_dict)\n",
    "            fout.write(str(tgt_pid)+' ')\n",
    "            src_str=''\n",
    "            for index in src_sent:\n",
    "                src_str+=str(index)\n",
    "                src_str+=' '\n",
    "            src_str=src_str[:-1]\n",
    "            tgt_str=''\n",
    "            for index in tgt_sent:\n",
    "                tgt_str+=str(index)\n",
    "                tgt_str+=' '\n",
    "            tgt_str=tgt_str[:-1]\n",
    "            fout.write(src_str)\n",
    "            fout.write('|')\n",
    "            fout.write(str(tgt_pid)+' ')\n",
    "            fout.write(tgt_str+'\\n')\n",
    "    print('finish',file,len(src),'')\n",
    "            \n",
    "def output_vocab_file(index_vocab_dict,file):\n",
    "    with open(file,'w') as fout:\n",
    "        for i in range(1,len(index_vocab_dict)+1):\n",
    "            word=index_vocab_dict[i]\n",
    "            fout.write(word+'\\n')    \n",
    "    print(len(index_vocab_dict),'words saved in',file)            \n",
    "    \n",
    "# 4. also encode the file\n",
    "def back_encode_sentence(inds,index_vocab_dict):\n",
    "    sent=''\n",
    "    for ind in inds:\n",
    "        sent+=index_vocab_dict[ind]\n",
    "        sent+=' '\n",
    "    return sent[:-1]\n",
    "def encode_output(src,tgt,index_vocab_dict,file):\n",
    "    with open(file,'w') as fout:\n",
    "        assert len(src)==len(tgt)\n",
    "        for i in range(len(src)):\n",
    "            src_sent=src[i]\n",
    "            pair=tgt[i]\n",
    "            tgt_sent=pair['line']\n",
    "            tgt_pid=pair['pid']\n",
    "            src_sent=encode_sentence(src_sent,vocab_index_dict)\n",
    "            tgt_sent=encode_sentence(tgt_sent,vocab_index_dict)\n",
    "            src_sent=back_encode_sentence(src_sent,index_vocab_dict)\n",
    "            tgt_sent=back_encode_sentence(tgt_sent,index_vocab_dict)\n",
    "            output=str(tgt_pid)+' '+src_sent+'￨'+str(tgt_pid)+' '+tgt_sent+'\\n'\n",
    "            fout.write(output)\n",
    "    print('finish',file,'with',len(src),'lines')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "\n",
    "vocab_dict={}\n",
    "file=input_path+'/src-test.txt'\n",
    "src_test,vocab_dict=read_src_data(file,vocab_dict)\n",
    "file=input_path+'/src-train.txt'\n",
    "src_train,vocab_dict=read_src_data(file,vocab_dict)\n",
    "file=input_path+'/src-val.txt'\n",
    "src_val,vocab_dict=read_src_data(file,vocab_dict)\n",
    "\n",
    "\n",
    "\n",
    "file=input_path+'/src-test-full.txt'\n",
    "src_test_full,vocab_dict=read_src_data(file,vocab_dict)\n",
    "file=input_path+'/src-val-full.txt'\n",
    "src_val_full,vocab_dict=read_src_data(file,vocab_dict)\n",
    "\n",
    "\n",
    "file=input_path+'/tgt-test.txt'\n",
    "tgt_test,vocab_dict=read_tgt_data(file,vocab_dict)\n",
    "file=input_path+'/tgt-test-full.txt'\n",
    "tgt_test_full,vocab_dict=read_tgt_data(file,vocab_dict)\n",
    "\n",
    "file=input_path+'/tgt-val.txt'\n",
    "tgt_val,vocab_dict=read_tgt_data(file,vocab_dict)\n",
    "file=input_path+'/tgt-val-full.txt'\n",
    "tgt_val_full,vocab_dict=read_tgt_data(file,vocab_dict)\n",
    "\n",
    "file=input_path+'/tgt-train.txt'\n",
    "tgt_train,vocab_dict=read_tgt_data(file,vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words num in vocab: 42182\n"
     ]
    }
   ],
   "source": [
    "# 2. build vocab\n",
    "\n",
    "vocab_index_dict,index_vocab_dict=build_vocab(vocab_dict,min_frequency=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish ../data/src_data_full_feat_tf_resplited_review/test.txt 4167 \n",
      "finish ../data/src_data_full_feat_tf_resplited_review/train.txt 960114 \n",
      "finish ../data/src_data_full_feat_tf_resplited_review/valid.txt 4167 \n",
      "42182 words saved in ../data/src_data_full_feat_tf_resplited_review/vocabulary\n"
     ]
    }
   ],
   "source": [
    "# 3. output file\n",
    "\n",
    "output_one_set(src_test,tgt_test,file='../data/src_data_full_feat_tf_resplited_review/test.txt')\n",
    "\n",
    "output_one_set(src_train,tgt_train,file='../data/src_data_full_feat_tf_resplited_review/train.txt')\n",
    "\n",
    "output_one_set(src_val,tgt_val,file='../data/src_data_full_feat_tf_resplited_review/valid.txt')\n",
    "\n",
    "output_vocab_file(index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review/vocabulary',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish ../data/src_data_full_feat_tf_resplited_review/test_ref.txt with 4167 lines\n",
      "finish ../data/src_data_full_feat_tf_resplited_review/val_ref.txt with 4167 lines\n",
      "finish ../data/src_data_full_feat_tf_resplited_review/train_ref.txt with 960114 lines\n"
     ]
    }
   ],
   "source": [
    "# 4. also encode the file\n",
    "encode_output(src_test,tgt_test,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review/test_ref.txt')\n",
    "encode_output(src_val,tgt_val,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review/val_ref.txt')\n",
    "encode_output(src_train,tgt_train,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review/train_ref.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words num in vocab: 49844\n"
     ]
    }
   ],
   "source": [
    "# 2. build vocab\n",
    "\n",
    "vocab_index_dict,index_vocab_dict=build_vocab(vocab_dict,min_frequency=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/test.txt 4167 \n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/test_full.txt 16668 \n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/valid.txt 4167 \n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/valid_full.txt 16668 \n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/train.txt 960114 \n",
      "49844 words saved in ../data/src_data_full_feat_tf_resplited_review_50k/vocabulary\n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/test_ref.txt with 4167 lines\n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/val_ref.txt with 4167 lines\n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/test_full_ref.txt with 16668 lines\n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/val_full_ref.txt with 16668 lines\n",
      "finish ../data/src_data_full_feat_tf_resplited_review_50k/train_ref.txt with 960114 lines\n"
     ]
    }
   ],
   "source": [
    "# 3. output file\n",
    "\n",
    "output_one_set(src_test,tgt_test,file='../data/src_data_full_feat_tf_resplited_review_50k/test.txt')\n",
    "\n",
    "output_one_set(src_test_full,tgt_test_full,file='../data/src_data_full_feat_tf_resplited_review_50k/test_full.txt')\n",
    "\n",
    "output_one_set(src_val,tgt_val,file='../data/src_data_full_feat_tf_resplited_review_50k/valid.txt')\n",
    "\n",
    "output_one_set(src_val_full,tgt_val_full,file='../data/src_data_full_feat_tf_resplited_review_50k/valid_full.txt')\n",
    "\n",
    "output_one_set(src_train,tgt_train,file='../data/src_data_full_feat_tf_resplited_review_50k/train.txt')\n",
    "\n",
    "\n",
    "\n",
    "output_vocab_file(index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review_50k/vocabulary',)\n",
    "\n",
    "# 4. also encode the file\n",
    "encode_output(src_test,tgt_test,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review_50k/test_ref.txt')\n",
    "encode_output(src_val,tgt_val,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review_50k/val_ref.txt')\n",
    "encode_output(src_test_full,tgt_test_full,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review_50k/test_full_ref.txt')\n",
    "encode_output(src_val_full,tgt_val_full,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review_50k/val_full_ref.txt')\n",
    "\n",
    "encode_output(src_train,tgt_train,index_vocab_dict,file='../data/src_data_full_feat_tf_resplited_review_50k/train_ref.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py \\\n",
    "# --data_folder 'data/src_data_full_feat_tf_resplited_review' \\\n",
    "# --SpeakerMode --batch_size 1 --eval_steps 1 --layers 1 --no_save\n",
    "\n",
    "\n",
    "# python train.py \\\n",
    "# --data_folder 'data/src_data_full_feat_tf_resplited_review' \\\n",
    "# --SpeakerMode --batch_size 1 --eval_steps 1 --layers 1 --dev_file 'valid_testing.txt'  --no_save\n",
    "\n",
    "\n",
    "# python train.py \\\n",
    "# --data_folder 'data/src_data_full_feat_tf_resplited_review' \\\n",
    "# --SpeakerMode --batch_size 16 --max_iter 300000 --layers 3 --no_save\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review' --SpeakerMode \\\n",
    "--batch_size 200 --layers 2 --epochs 10 --eval_steps 700 --save_steps 2 --train_size 960114\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review' \\\n",
    "--batch_size 200 --layers 2 --epochs 10 --eval_steps 700 --save_steps 2 --train_size 960114\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review' --SpeakerMode \\\n",
    "--batch_size 200 --layers 2 --epochs 10 --eval_steps 700 --save_steps 2 --train_size 960114\n",
    "\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--save_folder 'save/s_rand' \\\n",
    "--save_prefix 'dim_40_lstm_512_layer_2.model.' \\\n",
    "--output_file 's_rand_dim_40_lstm_512_layer_2.log' \\\n",
    "--SpeakerMode \\\n",
    "--batch_size 200 --layers 2 --epochs 10 --eval_steps 700 --save_steps 2 --train_size 960114 \\\n",
    "--PersonaDim 40 --debug\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python decode.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review' \\\n",
    "--model_folder 'save/testing' --model_name 'model10' --params_name 'params' \\\n",
    "--output_folder 'outputs' --SpeakerMode --output_file 'SpeakerID1A1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--save_folder 'save/s_rand' \\\n",
    "--save_prefix 'dim_30_lstm_512_layer_2.model.' \\\n",
    "--output_file 's_rand_dim_30_lstm_512_layer_2.log' \\\n",
    "--SpeakerMode \\\n",
    "--batch_size 200 --layers 2 --epochs 10 --eval_steps 700 --save_steps 2 --train_size 960114 \\\n",
    "--PersonaDim 30 \n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python decode.py --decode_file 'test.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_rand' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--output_folder 'outputs/s_rand' --SpeakerMode --output_file 's_rand_dim_30_lstm_512_layer_2_debug.txt'\n",
    "CUDA_VISIBLE_DEVICES=0 python decode.py --decode_file 'test_full.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_rand' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--output_folder 'outputs/s_rand' --SpeakerMode --output_file 's_rand_dim_30_lstm_512_layer_2_debug.txt'\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python decode.py --decode_file 'valid.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_rand' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--output_folder 'outputs/s_rand' --SpeakerMode --output_file 's_rand_dim_30_lstm_512_layer_2_debug.txt'\n",
    "CUDA_VISIBLE_DEVICES=0 python decode.py --decode_file 'valid_full.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_rand' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--output_folder 'outputs/s_rand' --SpeakerMode --output_file 's_rand_dim_30_lstm_512_layer_2_debug.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--save_folder 'save/s_tf' \\\n",
    "--save_prefix 'dim_30_lstm_512_layer_2.model.' \\\n",
    "--output_file 's_tf_dim_30_lstm_512_layer_2.log' \\\n",
    "--SpeakerMode \\\n",
    "--PersonaEmbFiles '../PerGen/src_data_full_1M_PCA_dim_384/FACTOR2_iter_200_15x30x20_0.0005_1_4' \\\n",
    "--batch_size 200 --layers 2 --epochs 10 --eval_steps 700 --save_steps 2 --train_size 960114 \\\n",
    "--PersonaDim 30 \n",
    "\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 python decode.py --decode_file 'test.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_tf' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--SpeakerMode  \\\n",
    "--output_folder 'outputs/s_tf' --output_file 's_tf_dim_30_lstm_512_layer_2.txt'\n",
    "CUDA_VISIBLE_DEVICES=1 python decode.py --decode_file 'test_full.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_tf' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--SpeakerMode  \\\n",
    "--output_folder 'outputs/s_tf' --output_file 's_tf_dim_30_lstm_512_layer_2.txt'\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 python decode.py --decode_file 'valid.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_tf' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--SpeakerMode  \\\n",
    "--output_folder 'outputs/s_tf' --output_file 's_tf_dim_30_lstm_512_layer_2.txt'\n",
    "CUDA_VISIBLE_DEVICES=1 python decode.py --decode_file 'valid_full.txt' \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--model_folder 'save/s_tf' --model_name 'dim_30_lstm_512_layer_2.model.best' --params_name 'params' \\\n",
    "--SpeakerMode  \\\n",
    "--output_folder 'outputs/s_tf' --output_file 's_tf_dim_30_lstm_512_layer_2.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py \\\n",
    "--data_folder 'data/src_data_full_feat_tf_resplited_review_50k' \\\n",
    "--save_folder 'save/seq2seq' \\\n",
    "--save_prefix 'lstm_512_layer_2.model.' \\\n",
    "--output_file 'seq2seq_lstm_512_layer_2.log' \\\n",
    "--batch_size 400 --layers 2 --epochs 10 --eval_steps 350 --save_steps 2 --train_size 960114 \\\n",
    "--gpu '0,1,2,3'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
