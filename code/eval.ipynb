{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tgt_data(file):\n",
    "    tgts=[]\n",
    "    with open(file,'r') as fin:\n",
    "        lines=fin.readlines()\n",
    "        for line in lines:\n",
    "            src,tgt=line.strip().split('ï¿¨')\n",
    "            tgt_=tgt.split(' ')[1:]\n",
    "            tgt=''\n",
    "            for word in tgt_:\n",
    "                tgt+=word\n",
    "                tgt+=' '\n",
    "            tgts.append(tgt[:-1])\n",
    "    return tgts\n",
    "def cal_bleu(ref_file,output_file):\n",
    "    refs=read_tgt_data(file=ref_file)\n",
    "    outputs=read_tgt_data(file=output_file)\n",
    "    # TODO: figure out what is ref and what is hypothethis.\n",
    "    bleu = sacrebleu.corpus_bleu(outputs, [refs])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9998276939151607"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_file='../data/src_data_full_feat_tf_resplited_review/test_ref.txt'\n",
    "output_file='../outputs/test.txt_Speaker_output.txt'\n",
    "cal_bleu(ref_file,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1920700187852555"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_file='../data/src_data_full_feat_tf_resplited_review/test_ref.txt'\n",
    "output_file='../outputs/test.txt_Speaker_SpeakerID1A1'\n",
    "cal_bleu(ref_file,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
